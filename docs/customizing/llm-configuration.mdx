---
title: LLM Configuration
description: "Visually grounded LLMs compatible with Magnitude"
icon: brain-cog
---

Magnitude requires a **visually grounded** LLM - a model that understands precise coordinates in an image so it can interact with the browser accurately.

<Warning> Most LLMs are NOT grounded, for example models from OpenAI, Gemini, or Llama. </Warning>

The easiest grounded model to set up with Magnitude is **Claude Sonnet 4** from Anthropic. To use this model, simply set `ANTHROPIC_API_KEY` in your environment. To configure a different grounded model, see below.

<Info> You can also choose a specific Claude model by configuring an [Anthropic](/reference/llm-providers#anthropic) or [Bedrock](/reference/llm-providers#aws-bedrock) client in Magnitude config. </Info>

## Other compatible models

Other models such as Qwen 2.5 VL 72B can achieve comparable performance to Claude at a lower cost.

Here's an example of how you could configure Magnitude to use Qwen 2.5 VL 72B via OpenRouter:

```typescript magnitude.config.ts
import { type MagnitudeConfig } from 'magnitude-test';

export default {
    url: "http://localhost:5173",
    planner: {
        provider: 'openai-generic',
        options: {
            baseUrl: 'https://openrouter.ai/api/v1',
            model: 'qwen/qwen2.5-vl-72b-instruct',
            apiKey: process.env.OPENROUTER_API_KEY
        }
    }
} satisfies MagnitudeConfig;
```

For instructions on configuring LLMs with various providers, see [LLM Providers](/reference/llm-providers).

Some other grounded models include but are not limited to:
- Qwen 2.5 VL [32B](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct), [7B](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) and [3B](huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- [UI-TARS-1.5-7B](https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B)
- [Molmo-72B](https://huggingface.co/allenai/Molmo-72B-0924)
- [Jedi-7B](https://huggingface.co/xlangai/Jedi-7B-1080p)
- [Holo1-7B](https://huggingface.co/Hcompany/Holo1-7B)


<Warning> Use smaller models at your own peril - typically they will not do a good job of running test cases. Models in the 72B+ range are recommended. </Warning>

