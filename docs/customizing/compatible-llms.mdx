---
title: Compatible LLMs
description: "Grounded LLMs compatible with Magnitude"
icon: brain-cog
---

Magnitude requires a **grounded** LLM such that the precise coordinates of elements on the browser are known and can be interacted with accurately.

Not all LLMs are grounded, in fact most aren't. Let's break down what these models are and which you might use.

## What is a grounded LLM?

A **grounded** LLM is a multi-modal vision model that given an image, knows precisely the pixel coordinates of any element in that image.

Typically, LLMs are only grounded if they are specifically trained for this.

Notably, popular models like any model from OpenAI, Gemini, Llama, and others are NOT grounded, and so it is not recommended to use them with Magnitude.

## Grounded models

### (Recommended) Anthropic Claude Sonnet 4
To use: set `ANTHROPIC_API_KEY` environment variable, or configure an [Anthropic](/reference/llm-configuration#anthropic) or [Bedrock](/reference/llm-configuration#aws-bedrock) client in Magnitude config.

### (Alternative) Qwen 2.5 VL 72B
To use: set `OPENROUTER_API_KEY` environment variable, or configure a [OpenAI generic](/reference/llm-configuration#openai-compatible) client in Magnitude config that points to any inference provider or locally running instance serving this.

### Other Models

Some other grounded models include but are not limited to:
- Qwen 2.5 VL [32B](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct), [7B](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) and [3B](huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- [UI-TARS-1.5-7B](https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B)
- [Molmo-72B](https://huggingface.co/allenai/Molmo-72B-0924)
- [Jedi-7B](https://huggingface.co/xlangai/Jedi-7B-1080p)
- [Holo1-7B](https://huggingface.co/Hcompany/Holo1-7B)


<Warning> Use smaller models at your own peril - typically they will not do a good job of running test cases. Models in the 72B+ range are recommended. </Warning>

